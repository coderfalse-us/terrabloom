{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prZeDyhx74kj",
        "outputId": "a8126ee5-e93e-4c5c-ecd8-6a6bdd7c2d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        " pip install langchain_openai langchain-google-genai langchain_community langchain pymysql chromadb langchain_chroma -q \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "dTVGoMAHbqkq",
        "outputId": "5438e8d7-8daf-4dd1-af06-89dff29e512f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting google-ai-generativelanguage==0.6.15\n",
            "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-ai-generativelanguage==0.6.15) (2.40.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-ai-generativelanguage==0.6.15) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-ai-generativelanguage==0.6.15) (5.29.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (2024.8.30)\n",
            "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "Installing collected packages: google-ai-generativelanguage\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.18\n",
            "    Uninstalling google-ai-generativelanguage-0.6.18:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.18\n",
            "Successfully installed google-ai-generativelanguage-0.6.15\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.4 requires google-ai-generativelanguage<0.7.0,>=0.6.18, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install google-ai-generativelanguage==0.6.15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyBc_8Ls8yQQsgOgeMusRW3Y8jcC3EO1E_k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EEK88-fhG3zx"
      },
      "outputs": [],
      "source": [
        "# Set environment variables for Gemini API\n",
        "GEMINI_API_KEY=\"AIzaSyCKHLCrRFIlREEr37RMuqf83E0ezWxdghY\"\n",
        "GEMINI_ENDPOINT=\"https://api.google.com/gemini\"  # Replace with Gemini's actual endpoint\n",
        "LANGSMITH_TRACING=True\n",
        "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "LANGSMITH_API_KEY=\"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\"\n",
        "LANGSMITH_PROJECT=\"pr-memorable-spume-62\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=\"C:\\\\Users\\\\christo.jomon\\\\Downloads\\\\RAG\\\\api-test-459905-8577acea1327.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r6Y9BD_3G6qh",
        "outputId": "14f28ae2-90d4-44b8-bebf-c3bbc0141899"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--33fbd051-d5b2-438e-86ce-9bca9f50a2d8-0', usage_metadata={'input_tokens': 4, 'output_tokens': 11, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0\n",
        "    # Don't pass credentials here when using API key\n",
        ")\n",
        "llm.invoke(\"Hello, world!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "v_MDy4HkQyDv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'export' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'export' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!export LANGSMITH_TRACING=True\n",
        "!export LANGSMITH_API_KEY=\"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XpgD0se8O6KI"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGSMITH_TRACING\"]=\"True\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"]=\"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\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"]=\"pr-gargantuan-passing-22\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2QpnL5i8MK-",
        "outputId": "dc8250da-4821-40a8-d8f6-544358dd86d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "postgresql\n",
            "['accounts', 'building', 'businessunits', 'container', 'customers', 'locationorganizations']\n",
            "\n",
            "CREATE TABLE customersetup.accounts (\n",
            "\tid VARCHAR(50) NOT NULL, \n",
            "\tcustomerid VARCHAR(50), \n",
            "\tbusinessunitid VARCHAR(50), \n",
            "\tbusinessunitnumber VARCHAR(50), \n",
            "\taccountcode VARCHAR(15), \n",
            "\tdescription VARCHAR(50), \n",
            "\tcurrency VARCHAR(50), \n",
            "\tcurrencyid VARCHAR(50), \n",
            "\tmetrics VARCHAR(50), \n",
            "\tmetricsid VARCHAR(50), \n",
            "\tisactive CHAR(1), \n",
            "\tactivedate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tinactivedate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tisaggregate CHAR(1), \n",
            "\tcreatedby VARCHAR(250), \n",
            "\tcreateddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tmodifiedby VARCHAR(250), \n",
            "\tmodifieddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\ttimeinepoch NUMERIC, \n",
            "\tisdeleted CHAR(1), \n",
            "\tisarchived CHAR(1), \n",
            "\tsynapsefacilityname VARCHAR(40), \n",
            "\tenvironmentcode VARCHAR(40), \n",
            "\tisverticaldefault CHAR(1), \n",
            "\tcurrentappversion VARCHAR(7), \n",
            "\tmodifiedappversion VARCHAR(7), \n",
            "\tcreatedlocation VARCHAR(200), \n",
            "\tupdatedlocation VARCHAR(200), \n",
            "\treturnaddress1 VARCHAR(200), \n",
            "\treturnaddress2 VARCHAR(200), \n",
            "\treturnaddress3 VARCHAR(200), \n",
            "\treturncity VARCHAR(150), \n",
            "\treturnstatecode VARCHAR(150), \n",
            "\treturnpostalcode VARCHAR(15), \n",
            "\treturncountrycode VARCHAR(2), \n",
            "\tallowreturns CHAR(1), \n",
            "\tallowexchange CHAR(1), \n",
            "\tdisposition CHAR(1), \n",
            "\tsynapsecustomerid VARCHAR(40), \n",
            "\tenableprescript CHAR(1), \n",
            "\tenablepostscript CHAR(1), \n",
            "\tisadvanced CHAR(1), \n",
            "\tnumberofshift NUMERIC, \n",
            "\tlineofbusiness VARCHAR(50), \n",
            "\taccountid VARCHAR(40), \n",
            "\temergencycontractnumber VARCHAR(25), \n",
            "\temergencydomesticcontact VARCHAR(25), \n",
            "\temergencyintlcontact VARCHAR(25), \n",
            "\treturncontactname VARCHAR(100), \n",
            "\treturnwindow INTEGER, \n",
            "\tCONSTRAINT account_id PRIMARY KEY (id)\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from accounts table:\n",
            "id\tcustomerid\tbusinessunitid\tbusinessunitnumber\taccountcode\tdescription\tcurrency\tcurrencyid\tmetrics\tmetricsid\tisactive\tactivedate\tinactivedate\tisaggregate\tcreatedby\tcreateddate\tmodifiedby\tmodifieddate\ttimeinepoch\tisdeleted\tisarchived\tsynapsefacilityname\tenvironmentcode\tisverticaldefault\tcurrentappversion\tmodifiedappversion\tcreatedlocation\tupdatedlocation\treturnaddress1\treturnaddress2\treturnaddress3\treturncity\treturnstatecode\treturnpostalcode\treturncountrycode\tallowreturns\tallowexchange\tdisposition\tsynapsecustomerid\tenableprescript\tenablepostscript\tisadvanced\tnumberofshift\tlineofbusiness\taccountid\temergencycontractnumber\temergencydomesticcontact\temergencyintlcontact\treturncontactname\treturnwindow\n",
            "01HNWJ3KX8G7SDH0MVZB6VMEGY\t01HGZHVEF2EYYQ8DBPAPVF908J\t01HNWHZDKA9TEX651V19R5JGG7\t17171\tqaaa\tQAAA\tUSD\tAB3FE67977324FD08C2AD003636BC0A1\tUSC\tQDFTDGQNBL22LR0JXJZCUCKFSGWLSWU2\t1\t2024-02-05 11:55:48\tNone\t0\tqaautomation\t2024-02-05 11:57:56.455000\tNone\tNone\t1707134276\t0\t0\t\tPROD1\t0\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\t1\t0\t0\tNone\t0\t0\tNone\tNone\tNone\t01HNWJ3KX8G7SDH0MVZB6VMEGY\tNone\tNone\tNone\tNone\tNone\n",
            "01GR9QT6DGQEF5M8AR5ZW30E8S\t01GR9QQXCPZZZZY48676RQBVDQ\t01GR9QS7RCMBMKWZRY9QMGZTTT\t10000\tGEC\teLogistics Commerce\tUSD\tAB3FE67977324FD08C2AD003636BC0A1\tSI\t30S60T3AYYPGYENXKWMP7ZXT53JYFJGP\t1\t2023-02-02 06:56:25\tNone\t0\thsankepally\t2023-02-02 18:56:51.361000\tsnunnavath\t2023-07-05 19:45:47.131000\t1688586347\t0\t0\tGEC\tPROD1\t0\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\t1\t0\t0\t GEODIS eLogistics\t0\t0\tNone\tNone\tNone\t01GR9QT6DGQEF5M8AR5ZW30E8S\tNone\tNone\tNone\tNone\tNone\n",
            "01H4487WC5WY8NC39CRYQZ89ER\t01H42Z94M9AHM7AHZMTXV7T27C\t01H446G0QDCR4VPFWD0C5C9EEV\tlkhlikhj\ttex\tcsdcs\tUSD\tAB3FE67977324FD08C2AD003636BC0A1\tSI\t30S60T3AYYPGYENXKWMP7ZXT53JYFJGP\t1\t2023-06-29 18:56:10\tNone\t0\tx-hborra\t2023-06-29 18:56:52.543000\tNone\tNone\t1688065012\t0\t0\tNone\tEnvironment1\t0\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\t1\t0\t0\tNone\t0\t0\tNone\tNone\tNone\t01H4487WC5WY8NC39CRYQZ89ER\tNone\tNone\tNone\tNone\tNone\n",
            "*/\n",
            "\n",
            "\n",
            "CREATE TABLE customersetup.building (\n",
            "\tid VARCHAR(50) NOT NULL, \n",
            "\tbuildingcode VARCHAR(50) NOT NULL, \n",
            "\taddress1 VARCHAR(200) NOT NULL, \n",
            "\taddress2 VARCHAR(200), \n",
            "\tcity VARCHAR(35) NOT NULL, \n",
            "\tstate VARCHAR(2) NOT NULL, \n",
            "\tstatedescription VARCHAR(40) NOT NULL, \n",
            "\tpostalcode VARCHAR(10) NOT NULL, \n",
            "\tcountry VARCHAR(3) NOT NULL, \n",
            "\tcountrydescription VARCHAR(50), \n",
            "\ttimezone VARCHAR(50) NOT NULL, \n",
            "\tphonenumber VARCHAR(20), \n",
            "\tisactive CHAR(1), \n",
            "\tactivedate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tinactivedate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tcreatedby VARCHAR(250), \n",
            "\tcreateddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tmodifiedby VARCHAR(250), \n",
            "\tmodifieddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\ttimeinepoch NUMERIC, \n",
            "\tisdeleted CHAR(1), \n",
            "\tisarchived CHAR(1), \n",
            "\tcurrentappversion VARCHAR(7), \n",
            "\tmodifiedappversion VARCHAR(7), \n",
            "\tcreatedlocation VARCHAR(200), \n",
            "\tupdatedlocation VARCHAR(200), \n",
            "\tbuildingdescription VARCHAR(200), \n",
            "\tbusinessunitid VARCHAR(50), \n",
            "\taccountid VARCHAR(50), \n",
            "\tcustomerid VARCHAR(50), \n",
            "\twarehouseid VARCHAR(50), \n",
            "\ttimezonedescription VARCHAR(40), \n",
            "\taddress3 VARCHAR(50), \n",
            "\tCONSTRAINT building_id PRIMARY KEY (id)\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from building table:\n",
            "id\tbuildingcode\taddress1\taddress2\tcity\tstate\tstatedescription\tpostalcode\tcountry\tcountrydescription\ttimezone\tphonenumber\tisactive\tactivedate\tinactivedate\tcreatedby\tcreateddate\tmodifiedby\tmodifieddate\ttimeinepoch\tisdeleted\tisarchived\tcurrentappversion\tmodifiedappversion\tcreatedlocation\tupdatedlocation\tbuildingdescription\tbusinessunitid\taccountid\tcustomerid\twarehouseid\ttimezonedescription\taddress3\n",
            "01JTMFRCY4FM669FHK90AFRSJD\ty3nfwacw\t124 Main Street\tPO Box 124\tBrentwood\tAL\tAlabama\t37027\tUS\tUnited States of America\tAmerica/Chicago\tNone\t1\t2025-05-07 00:00:00\tNone\tqaautomation\t2025-05-07 04:23:42.821000\tNone\tNone\t1746591822\t0\t0\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tCentral Standard Time\t7th Block\n",
            "01JCFW86KTVQK1KPTF73GTF7G5\tblcV\taddressline1 ofblcV\taddress line 2 ofblcV\tErnakulam\tTN\tTennessee\t682037\tUS\tUnited States of America\tEtc/GMT+8\t12345\t1\t2024-11-12 09:45:56\tNone\tqaautomation\t2024-11-12 09:46:05.846000\tNone\tNone\t1731404765\t0\t0\tNone\tNone\tNone\tNone\tbuildingblcV\tNone\tNone\tNone\tNone\tUTC-08\tNone\n",
            "01JCFZM3E51YYS4K5Y84SXMFEW\tblIi\taddressline1 of blIi\taddress line 2 of blIi\tErnakulam\tTN\tTennessee\t682037\tUS\tUnited States of America\tEtc/GMT+8\t12345\t1\t2024-11-12 10:44:51\tNone\tqaautomation\t2024-11-12 10:45:01.558000\tNone\tNone\t1731408301\t0\t0\tNone\tNone\tNone\tNone\tbuilding blIi\tNone\tNone\tNone\tNone\tUTC-08\tNone\n",
            "*/\n",
            "\n",
            "\n",
            "CREATE TABLE customersetup.businessunits (\n",
            "\tid VARCHAR(50) NOT NULL, \n",
            "\tcustomerid VARCHAR(50), \n",
            "\tbusinessunitnumber VARCHAR(50) NOT NULL, \n",
            "\tsynapsecustomername VARCHAR(50), \n",
            "\tactivedate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tinactivedate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tisactive CHAR(1), \n",
            "\tcreatedby VARCHAR(50), \n",
            "\tcreateddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tmodifiedby VARCHAR(50), \n",
            "\tmodifieddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\ttimeinepoch NUMERIC, \n",
            "\tisdeleted CHAR(1), \n",
            "\tchannelcode VARCHAR(4), \n",
            "\tverticalcode VARCHAR(4), \n",
            "\tverticalid VARCHAR(40), \n",
            "\tchannelid VARCHAR(40), \n",
            "\tbusinessunitname VARCHAR(100), \n",
            "\tisexternal CHAR(1), \n",
            "\tCONSTRAINT businessunits_pk PRIMARY KEY (id)\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from businessunits table:\n",
            "id\tcustomerid\tbusinessunitnumber\tsynapsecustomername\tactivedate\tinactivedate\tisactive\tcreatedby\tcreateddate\tmodifiedby\tmodifieddate\ttimeinepoch\tisdeleted\tchannelcode\tverticalcode\tverticalid\tchannelid\tbusinessunitname\tisexternal\n",
            "01JTJMP3SS8NFFPHBQ8F4Q7BAF\t01JTJMMHD7QRWMVXZMCHVH343K\tbuCfamGf\tNone\t2025-05-06 11:11:14\tNone\t1\tJJOHNY\t2025-05-06 11:11:21.965000\tNone\tNone\t1746529881\t0\tRET\tRET\t0517E0F5CA3B41B4A91CBEA3FD868CE0\t0517E0F5CA3B41B4A91CBEA3FD868CE0\tbubuCfamGf\t0\n",
            "01JNDHF6548AD9ZSVH0W36867C\t01JNDHE90YE7X107EHZ5HGV62W\tbuyt9eJ0\tNone\t2025-03-03 08:20:22\tNone\t1\tqaautomation\t2025-03-03 08:20:27.988000\tNone\tNone\t1740990027\t0\tRET\tRET\t0517E0F5CA3B41B4A91CBEA3FD868CE0\t0517E0F5CA3B41B4A91CBEA3FD868CE0\tbubuyt9eJ0\t0\n",
            "01JTJP3Y17SX3HZJPQ5JT5JYMK\t01JTJP2BYKM102GHP92QHESN6W\tbuAhMYek\tNone\t2025-05-06 11:36:15\tNone\t1\tJJOHNY\t2025-05-06 11:36:23.399000\tNone\tNone\t1746531383\t0\tRET\tRET\t0517E0F5CA3B41B4A91CBEA3FD868CE0\t0517E0F5CA3B41B4A91CBEA3FD868CE0\tbubuAhMYek\t0\n",
            "*/\n",
            "\n",
            "\n",
            "CREATE TABLE customersetup.container (\n",
            "\tid VARCHAR(40) NOT NULL, \n",
            "\tbusinessunitid VARCHAR(40), \n",
            "\twarehouseid VARCHAR(40), \n",
            "\taccountid VARCHAR(40), \n",
            "\tcontainerclass VARCHAR(40), \n",
            "\tcontainegroup VARCHAR(40), \n",
            "\tcontainername VARCHAR(40), \n",
            "\tdescription VARCHAR(40), \n",
            "\tvolumemin NUMERIC, \n",
            "\tvolumemax NUMERIC, \n",
            "\tvolumeuom VARCHAR(10), \n",
            "\tweightmin NUMERIC, \n",
            "\tweightmax NUMERIC, \n",
            "\tweightuom VARCHAR(10), \n",
            "\tunitmax NUMERIC, \n",
            "\tunituom VARCHAR(10), \n",
            "\tdimensionslength NUMERIC, \n",
            "\tdimensionswidth NUMERIC, \n",
            "\tdimensionsheight NUMERIC, \n",
            "\tdimensionsuom VARCHAR(10), \n",
            "\tcreatedby VARCHAR(250), \n",
            "\tcreateddate TIMESTAMP(6) WITHOUT TIME ZONE, \n",
            "\tmodifiedby VARCHAR(250), \n",
            "\tmodifieddate TIMESTAMP(6) WITHOUT TIME ZONE, \n",
            "\ttimeinepoch NUMERIC, \n",
            "\tisdeleted CHAR(1), \n",
            "\tisarchived CHAR(1), \n",
            "\tcurrentappversion VARCHAR(7), \n",
            "\tmodifiedappversion VARCHAR(7), \n",
            "\tcreatedlocation VARCHAR(200), \n",
            "\tupdatedlocation VARCHAR(200), \n",
            "\tcustomerid VARCHAR(50), \n",
            "\tCONSTRAINT container_pk PRIMARY KEY (id)\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from container table:\n",
            "id\tbusinessunitid\twarehouseid\taccountid\tcontainerclass\tcontainegroup\tcontainername\tdescription\tvolumemin\tvolumemax\tvolumeuom\tweightmin\tweightmax\tweightuom\tunitmax\tunituom\tdimensionslength\tdimensionswidth\tdimensionsheight\tdimensionsuom\tcreatedby\tcreateddate\tmodifiedby\tmodifieddate\ttimeinepoch\tisdeleted\tisarchived\tcurrentappversion\tmodifiedappversion\tcreatedlocation\tupdatedlocation\tcustomerid\n",
            "01GGY6SXXNFWE6W5YA5CBD9YE8\t60176\tNone\tSTB\tBAG\tA55C03DF805A4892A7C170FD7D2E4931\tNov321\tDescripttion \t100\t100\tCS\t13\t12\tCS\t12\tCS\t123\t1234\t12\tCM\tPostman\t2022-11-03 07:38:05.173000\tNone\tNone\t1667461085\t0\t0\tNone\tNone\tNone\tNone\tNone\n",
            "01GGYTDN9N2797RVTRK26EABJW\tNone\tNone\tNone\tTOTE\tA55C03DF805A4892A7C170FD7D2E4931\t112233\tDescripttion \t100\t100\tCS\t13\t12\tCS\t12\tCS\t123\t1234\t12\tCM\tPostman\t2022-11-03 13:20:54.632000\tNone\tNone\t1667481654\t0\t0\tNone\tNone\tNone\tNone\tNone\n",
            "01GGZF2Y3QKTRJ3Z0G3XR4ZV14\tNone\tNone\tNone\tBAG\tA55C03DF805A4892A7C170FD7D2E4931\tHA Group\tApple Finished goods\t100\t100\tEA\t13\t12\tEA\t12\tEA\t123\t1234\t12\tCM\tPostman\t2022-11-03 19:22:03.303000\tNone\tNone\t1667503323\t0\t0\tNone\tNone\tNone\tNone\tNone\n",
            "*/\n",
            "\n",
            "\n",
            "CREATE TABLE customersetup.customers (\n",
            "\tid VARCHAR(50) NOT NULL, \n",
            "\tname VARCHAR(50), \n",
            "\tabbreviation VARCHAR(50), \n",
            "\taddress1 VARCHAR(200), \n",
            "\taddress2 VARCHAR(200), \n",
            "\taddress3 VARCHAR(200), \n",
            "\tcity VARCHAR(35), \n",
            "\tstate VARCHAR(50), \n",
            "\tpostalcode VARCHAR(10), \n",
            "\tcountry VARCHAR(50), \n",
            "\tcontactname VARCHAR(100), \n",
            "\tcontactemail VARCHAR(100), \n",
            "\tcontactphone VARCHAR(100), \n",
            "\turl VARCHAR(200), \n",
            "\tlogo VARCHAR(200), \n",
            "\tmarketlineid VARCHAR(50), \n",
            "\tisactive CHAR(1), \n",
            "\tactivedate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tinactivedate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tisdataretention SMALLINT, \n",
            "\tpersonalretention INTEGER, \n",
            "\torderretention INTEGER, \n",
            "\tparcellabelretention INTEGER, \n",
            "\tismarketlineaccount SMALLINT, \n",
            "\tcreatedby VARCHAR(250), \n",
            "\tcreateddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tmodifiedby VARCHAR(250), \n",
            "\tmodifieddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\ttimeinepoch INTEGER, \n",
            "\tisdeleted CHAR(1), \n",
            "\tisarchived CHAR(1), \n",
            "\tcurrentappversion VARCHAR(7), \n",
            "\tmodifiedappversion VARCHAR(7), \n",
            "\tcreatedlocation VARCHAR(200), \n",
            "\tupdatedlocation VARCHAR(200), \n",
            "\tmarketlinedescription VARCHAR(100), \n",
            "\tmarketlineleader VARCHAR(100), \n",
            "\tmarketlineowner VARCHAR(100), \n",
            "\timageurl VARCHAR(150), \n",
            "\tCONSTRAINT customer_id PRIMARY KEY (id)\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from customers table:\n",
            "id\tname\tabbreviation\taddress1\taddress2\taddress3\tcity\tstate\tpostalcode\tcountry\tcontactname\tcontactemail\tcontactphone\turl\tlogo\tmarketlineid\tisactive\tactivedate\tinactivedate\tisdataretention\tpersonalretention\torderretention\tparcellabelretention\tismarketlineaccount\tcreatedby\tcreateddate\tmodifiedby\tmodifieddate\ttimeinepoch\tisdeleted\tisarchived\tcurrentappversion\tmodifiedappversion\tcreatedlocation\tupdatedlocation\tmarketlinedescription\tmarketlineleader\tmarketlineowner\timageurl\n",
            "01HPE6TR0NT9WSN0J34MNMEP23\tSRAM LLC\tSRAM\t1101 Whitaker Rd\t\tNone\tPlainfield\tIN\t46168-0000\tUS\tSRAM LLC\tTest@123.com\t880-880-8880\t\tNone\tNone\t1\t2024-02-12 08:20:53\tNone\t0\t0\t0\t0\t0\taathira\t2024-02-12 08:27:10.597000\tNone\tNone\t1707726430\t0\t0\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\n",
            "01JTJFA44YNP7R2JHAV90NZ2B1\tC0GJDZWX\tVIQU\taddressline1 of C0GJDZWX\taddressline2 of C0GJDZWX\taddressline3 of C0GJDZWX\tErnakulam\tKL\t682037\tIN\tcustomer C0GJDZWX\tc@geodis.com\t123\twww.cust.com\tNone\tNone\t1\t2025-05-06 09:37:10\tNone\t0\t0\t0\t0\t0\tJJOHNY\t2025-05-06 09:37:26.261000\tNone\tNone\t1746524246\t0\t0\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\n",
            "01GS5XQRAHZFV3WM3TPM6QQUHJ\tCU_BK4\tTest_4\t123 Main Street\tPO Box 123\tNone\tBrentwood\tTN\t1\tUSA\tDrishya\ttest@geodis.com\t12\thttp://mysite.org\tNone\tNone\t0\tNone\tNone\t0\t0\t0\t0\t0\ttestuser\t2023-02-13 17:39:06.885000\tNone\tNone\t1676309946\t0\t0\tNone\tNone\tNone\tNone\tNone\tNone\tNone\tNone\n",
            "*/\n",
            "\n",
            "\n",
            "CREATE TABLE customersetup.locationorganizations (\n",
            "\tid VARCHAR(50) NOT NULL, \n",
            "\tlocationid VARCHAR(50) NOT NULL, \n",
            "\torganizationid VARCHAR(50) NOT NULL, \n",
            "\tcreatedby VARCHAR(250), \n",
            "\tcreateddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\tmodifiedby VARCHAR(250), \n",
            "\tmodifieddate TIMESTAMP WITHOUT TIME ZONE, \n",
            "\ttimeinepoch NUMERIC, \n",
            "\tisdeleted CHAR(1), \n",
            "\tisarchived CHAR(1), \n",
            "\tcurrentappversion VARCHAR(7), \n",
            "\tmodifiedappversion VARCHAR(7), \n",
            "\tcreatedlocation VARCHAR(200), \n",
            "\tupdatedlocation VARCHAR(200), \n",
            "\tcustomerid VARCHAR(50), \n",
            "\taccountid VARCHAR(50), \n",
            "\tbusinessunitid VARCHAR(50), \n",
            "\twarehouseid VARCHAR(50), \n",
            "\taccountcode VARCHAR(15), \n",
            "\tbusinessunitnumber VARCHAR(50), \n",
            "\tCONSTRAINT locationorganizations_id PRIMARY KEY (id)\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from locationorganizations table:\n",
            "id\tlocationid\torganizationid\tcreatedby\tcreateddate\tmodifiedby\tmodifieddate\ttimeinepoch\tisdeleted\tisarchived\tcurrentappversion\tmodifiedappversion\tcreatedlocation\tupdatedlocation\tcustomerid\taccountid\tbusinessunitid\twarehouseid\taccountcode\tbusinessunitnumber\n",
            "01JQ4H8VY3A1MV15JZQJHXB045\t01JQ4H8VR6RZW0NQX9BGC9DTXF\t01JQ4FN5TCX7FQSPE076WCNDC2\tManjusha.Kunjamma@Geodis.com\t2025-03-24 16:55:14.793000\tNone\tNone\t1742835314\t0\t0\t2025-13\tNone\tNone\tNone\t01JQ4FAC7HREGY7SFFEC268PDB\t01JQ4FN5SPE07JP4DYMPZRKKTB\t01JQ4FHX222R718P755KCA57JJ\t01JQ4FTAYVPTM4ZAC0YQ8JBHV5\tMK1\tMK1\n",
            "01JHMMWHZXEQT15JZHTVQTX9X5\t01JHMMWHS2X0T4HTREDVBJDTH1\t01J84BG9NJDZ91VT3VV0QH2Q86\twh-admin\t2025-01-15 09:32:05.693000\tNone\tNone\t1736933525\t0\t0\t2025-03\tNone\tNone\tNone\t01J7EVBHN5WYE107B55084BKNH\t01J84BG9MB5NYXH0ST1S9HK7B9\t01J84B2PJPGQKDEYW21HAPGJF8\t01J84BYGEKY8V5BNTZBMYX4VKS\tFGT\t17361T\n",
            "01H4NKRS6FGET7KZ39NFK18D6S\t01H4NKRRF0A5F6NAZ1QNY4GYDK\t01H4NGBRWPT8YVH2KGQJXT1D3T\tsnunnavath\t2023-07-06 12:45:26.994000\tNone\tNone\t1688647526\t0\t0\tNone\tNone\tNone\tNone\t01GRKVSGV5517WAW1FSH6WP655\t01H4NGBRSKWS7FGD4MQDE641GR\t01H41J6FW71PT590TBH04VD0GK\t01H4A3B4AGYKTB7173BA6D0VDM\tTE5\tjlskjfoli\n",
            "*/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyBc_8Ls8yQQsgOgeMusRW3Y8jcC3EO1E_k\"\n",
        "\n",
        "db_user = \"avnadmin\"\n",
        "db_password = \"AVNS_XmWUvtwBa34zV7BHTuF\"\n",
        "db_host = \"pg-langchain-mikkelkhanwald1-2c4f.l.aivencloud.com\"\n",
        "db_name = \"defaultdb\"\n",
        "db_port = 27107\n",
        "db_schema = \"customersetup\"  # Define the schema name\n",
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Create the engine without specifying the schema in the URI\n",
        "engine = create_engine(f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\")\n",
        "\n",
        "# Create the SQLDatabase object, specifying the schema\n",
        "db = SQLDatabase(engine, schema=db_schema)\n",
        "print(db.dialect)\n",
        "print(db.get_usable_table_names())\n",
        "print(db.table_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8WseTbGHjHg",
        "outputId": "eae262a0-02c3-4930-a083-a8b733df3c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```sql\n",
            "SELECT\n",
            "  COUNT(*)\n",
            "FROM customersetup.accounts\n",
            "WHERE\n",
            "  \"currency\" = 'USD';\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import create_sql_query_chain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "\n",
        "# Instead of using default credentials, use an API key approach\n",
        "# Set your API key as an environment variable\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCKHLCrRFIlREEr37RMuqf83E0ezWxdghY\"  # Replace with your actual API key\n",
        "\n",
        "# Create the LLM without credentials parameter\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0\n",
        "    # Don't pass credentials here when using API key\n",
        ")\n",
        "\n",
        "# Assuming db is defined elsewhere in your code\n",
        "generate_query = create_sql_query_chain(llm, db)\n",
        "query = generate_query.invoke({\"question\": \"How many accounts have currency of usd\"})\n",
        "print(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "D7NVpfBKkvEX",
        "outputId": "d5241742-4bbf-4593-95f2-cebb84e0ab00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\christo.jomon\\AppData\\Local\\Temp\\ipykernel_26860\\1604731353.py:2: LangChainDeprecationWarning: The class `QuerySQLDataBaseTool` was deprecated in LangChain 0.3.12 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-community package and should be used instead. To use it run `pip install -U :class:`~langchain-community` and import as `from :class:`~langchain_community.tools import QuerySQLDatabaseTool``.\n",
            "  execute_query = QuerySQLDataBaseTool(db=db)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'[(199,)]'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.tools import QuerySQLDataBaseTool\n",
        "execute_query = QuerySQLDataBaseTool(db=db)\n",
        "def strip_sql_markdown(sql: str) -> str:\n",
        "    return sql.strip().replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "execute_query.invoke(strip_sql_markdown(query))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xTudeC-qku6Q",
        "outputId": "095cf2d7-ecd7-46a0-aa1c-f9b0cb2a92c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'There are 199 accounts with USD currency.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " from operator import itemgetter\n",
        " from langchain_core.runnables import RunnableLambda\n",
        " from langchain_core.output_parsers import StrOutputParser\n",
        " from langchain_core.prompts import PromptTemplate\n",
        " from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        " answer_prompt = PromptTemplate.from_template(\n",
        "     \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
        "\n",
        " Question: {question}\n",
        " SQL Query: {query}\n",
        " SQL Result: {result}\n",
        " Answer: \"\"\"\n",
        " )\n",
        "\n",
        " rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
        "\n",
        " chain = (\n",
        "    RunnablePassthrough.assign(query=generate_query).assign(\n",
        "        result=itemgetter(\"query\") | RunnableLambda(lambda q: execute_query.invoke(strip_sql_markdown(q)))\n",
        "    )\n",
        "    | rephrase_answer\n",
        " )\n",
        "\n",
        " chain.invoke({\"question\": \"How many accounts have currency of usd\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pandas in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pH8Pu0w-eSG9"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "import pandas as pd\n",
        "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "\n",
        "df = pd.read_csv(\"./table_schema.csv\")\n",
        "\n",
        "db_location = \"./chrome_langchain_db1\"\n",
        "add_documents = not os.path.exists(db_location)\n",
        "\n",
        "if add_documents:\n",
        "    documents = []\n",
        "    ids = []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        document = Document(\n",
        "            page_content=row[\"table_name\"] + \" \" + row[\"DDL\"],\n",
        "            id=str(i)\n",
        "        )\n",
        "        ids.append(str(i))\n",
        "        documents.append(document)\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"table_schema\",\n",
        "    persist_directory=db_location,\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "if add_documents:\n",
        "    vector_store.add_documents(documents=documents, ids=ids)\n",
        "\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_kwargs={\"k\": 1}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6PJXxNK_g3it"
      },
      "outputs": [],
      "source": [
        "schema=retriever.invoke(\"How many containers have currency of usd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='3', metadata={}, page_content='container CREATE TABLE customersetup.container (\\n\\tid varchar(40) NOT NULL,\\n\\tbusinessunitid varchar(40) NULL,\\n\\twarehouseid varchar(40) NULL,\\n\\taccountid varchar(40) NULL,\\n\\tcontainerclass varchar(40) NULL,\\n\\tcontainegroup varchar(40) NULL,\\n\\tcontainername varchar(40) NULL,\\n\\tdescription varchar(40) NULL,\\n\\tvolumemin numeric NULL,\\n\\tvolumemax numeric NULL,\\n\\tvolumeuom varchar(10) NULL,\\n\\tweightmin numeric NULL,\\n\\tweightmax numeric NULL,\\n\\tweightuom varchar(10) NULL,\\n\\tunitmax numeric NULL,\\n\\tunituom varchar(10) NULL,\\n\\tdimensionslength numeric NULL,\\n\\tdimensionswidth numeric NULL,\\n\\tdimensionsheight numeric NULL,\\n\\tdimensionsuom varchar(10) NULL,\\n\\tcreatedby varchar(250) NULL,\\n\\tcreateddate timestamp(6) NULL,\\n\\tmodifiedby varchar(250) NULL,\\n\\tmodifieddate timestamp(6) NULL,\\n\\ttimeinepoch numeric NULL,\\n\\tisdeleted bpchar(1) NULL,\\n\\tisarchived bpchar(1) NULL,\\n\\tcurrentappversion varchar(7) NULL,\\n\\tmodifiedappversion varchar(7) NULL,\\n\\tcreatedlocation varchar(200) NULL,\\n\\tupdatedlocation varchar(200) NULL,\\n\\tcustomerid varchar(50) NULL,\\n\\tCONSTRAINT container_pk PRIMARY KEY (id)\\n);')]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n0EsEKQDDysx",
        "outputId": "f3aa5f46-2dc3-4757-89ae-4d63c4ce7b29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'There are 199 accounts with USD currency.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " from operator import itemgetter\n",
        " from langchain_core.runnables import RunnableLambda\n",
        " from langchain_core.output_parsers import StrOutputParser\n",
        " from langchain_core.prompts import PromptTemplate\n",
        " from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        " answer_prompt = PromptTemplate.from_template(\n",
        "     \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
        "\n",
        " Question: {question}\n",
        " SQL Query: {query}\n",
        " SQL Result: {result}\n",
        " Answer: \"\"\"\n",
        " )\n",
        "\n",
        " rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
        "\n",
        " chain = (\n",
        "    RunnablePassthrough.assign(schema=lambda x: retriever.invoke(x[\"question\"])).assign(query=generate_query).assign(\n",
        "        result=itemgetter(\"query\") | RunnableLambda(lambda q: execute_query.invoke(strip_sql_markdown(q)))\n",
        "    )\n",
        "    | rephrase_answer\n",
        " )\n",
        "\n",
        " chain.invoke({\"question\": \"How many accounts have currency of usd\",\"schema\": schema})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(id='0', metadata={}, page_content='accounts CREATE TABLE customersetup.accounts (\\n        id varchar(50) NOT NULL,\\n        customerid varchar(50) NULL,\\n        businessunitid varchar(50) NULL,\\n        businessunitnumber varchar(50) NULL,\\n        accountcode varchar(15) NULL,\\n        description varchar(50) NULL,\\n        currency varchar(50) NULL,\\n        currencyid varchar(50) NULL,\\n        metrics varchar(50) NULL,\\n        metricsid varchar(50) NULL,\\n        isactive bpchar(1) NULL,\\n        activedate timestamp NULL,\\n        inactivedate timestamp NULL,\\n        isaggregate bpchar(1) NULL,\\n        createdby varchar(250) NULL,\\n        createddate timestamp NULL,\\n        modifiedby varchar(250) NULL,\\n        modifieddate timestamp NULL,\\n        timeinepoch numeric NULL,\\n        isdeleted bpchar(1) NULL,\\n        isarchived bpchar(1) NULL,\\n        synapsefacilityname varchar(40) NULL,\\n        environmentcode varchar(40) NULL,\\n        isverticaldefault bpchar(1) NULL,\\n        currentappversion varchar(7) NULL,\\n        modifiedappversion varchar(7) NULL,\\n        createdlocation varchar(200) NULL,\\n        updatedlocation varchar(200) NULL,\\n        returnaddress1 varchar(200) NULL,\\n        returnaddress2 varchar(200) NULL,\\n        returnaddress3 varchar(200) NULL,\\n        returncity varchar(150) NULL,\\n        returnstatecode varchar(150) NULL,\\n        returnpostalcode varchar(15) NULL,\\n        returncountrycode varchar(2) NULL,\\n        allowreturns bpchar(1) NULL,\\n        allowexchange bpchar(1) NULL,\\n        disposition bpchar(1) NULL,\\n        synapsecustomerid varchar(40) NULL,\\n        enableprescript bpchar(1) NULL,\\n        enablepostscript bpchar(1) NULL,\\n        isadvanced bpchar(1) NULL,\\n        numberofshift numeric NULL,\\n        lineofbusiness varchar(50) NULL,\\n        accountid varchar(40) NULL,\\n        emergencycontractnumber varchar(25) NULL,\\n        emergencydomesticcontact varchar(25) NULL,\\n        emergencyintlcontact varchar(25) NULL,\\n        returncontactname varchar(100) NULL,\\n        returnwindow int4 NULL,\\n        CONSTRAINT account_id PRIMARY KEY (id)\\n);'), Document(id='5', metadata={}, page_content='customer CREATE TABLE customersetup.customers (\\r\\n\\tid varchar(50) NOT NULL,\\r\\n\\t\"name\" varchar(50) NULL,\\r\\n\\tabbreviation varchar(50) NULL,\\r\\n\\taddress1 varchar(200) NULL,\\r\\n\\taddress2 varchar(200) NULL,\\r\\n\\taddress3 varchar(200) NULL,\\r\\n\\tcity varchar(35) NULL,\\r\\n\\tstate varchar(50) NULL,\\r\\n\\tpostalcode varchar(10) NULL,\\r\\n\\tcountry varchar(50) NULL,\\r\\n\\tcontactname varchar(100) NULL,\\r\\n\\tcontactemail varchar(100) NULL,\\r\\n\\tcontactphone varchar(100) NULL,\\r\\n\\turl varchar(200) NULL,\\r\\n\\tlogo varchar(200) NULL,\\r\\n\\tmarketlineid varchar(50) NULL,\\r\\n\\tisactive bpchar(1) NULL,\\r\\n\\tactivedate timestamp NULL,\\r\\n\\tinactivedate timestamp NULL,\\r\\n\\tisdataretention int2 NULL,\\r\\n\\tpersonalretention int4 NULL,\\r\\n\\torderretention int4 NULL,\\r\\n\\tparcellabelretention int4 NULL,\\r\\n\\tismarketlineaccount int2 NULL,\\r\\n\\tcreatedby varchar(250) NULL,\\r\\n\\tcreateddate timestamp NULL,\\r\\n\\tmodifiedby varchar(250) NULL,\\r\\n\\tmodifieddate timestamp NULL,\\r\\n\\ttimeinepoch int4 NULL,\\r\\n\\tisdeleted bpchar(1) NULL,\\r\\n\\tisarchived bpchar(1) NULL,\\r\\n\\tcurrentappversion varchar(7) NULL,\\r\\n\\tmodifiedappversion varchar(7) NULL,\\r\\n\\tcreatedlocation varchar(200) NULL,\\r\\n\\tupdatedlocation varchar(200) NULL,\\r\\n\\tmarketlinedescription varchar(100) NULL,\\r\\n\\tmarketlineleader varchar(100) NULL,\\r\\n\\tmarketlineowner varchar(100) NULL,\\r\\n\\timageurl varchar(150) NULL,\\r\\n\\tCONSTRAINT customer_id PRIMARY KEY (id)\\r\\n);'), Document(id='2', metadata={}, page_content='businessunits CREATE TABLE customersetup.businessunits (\\n\\tid varchar(50) NOT NULL,\\n\\tcustomerid varchar(50) NULL,\\n\\tbusinessunitnumber varchar(50) NOT NULL,\\n\\tsynapsecustomername varchar(50) NULL,\\n\\tactivedate timestamp NULL,\\n\\tinactivedate timestamp NULL,\\n\\tisactive bpchar(1) NULL,\\n\\tcreatedby varchar(50) NULL,\\n\\tcreateddate timestamp NULL,\\n\\tmodifiedby varchar(50) NULL,\\n\\tmodifieddate timestamp NULL,\\n\\ttimeinepoch numeric NULL,\\n\\tisdeleted bpchar(1) NULL,\\n\\tchannelcode varchar(4) NULL,\\n\\tverticalcode varchar(4) NULL,\\n\\tverticalid varchar(40) NULL,\\n\\tchannelid varchar(40) NULL,\\n\\tbusinessunitname varchar(100) NULL,\\n\\tisexternal bpchar(1) NULL,\\n\\tCONSTRAINT businessunits_pk PRIMARY KEY (id)\\n);')]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'There are 0 unique synapsecustomername values in the customersetup.businessunits table.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0,\n",
        "    max_output_tokens=1024\n",
        ")\n",
        "def safe_execute_query(query):\n",
        "    try:\n",
        "        return execute_query.invoke(strip_sql_markdown(query))\n",
        "    except Exception as e:\n",
        "        return f\"Error executing query: {str(e)}\"\n",
        "\n",
        "\n",
        "def generate_system_message():\n",
        "    return {\n",
        "        \"role\": \"system\", \n",
        "        \"content\": \"You are a data analyst who provides definitive, precise answers based on SQL query results. Never hedge or express uncertainty unless the data is incomplete.\"\n",
        "    }\n",
        "\n",
        "\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"You are a data analyst who provides definitive, precise answers based on SQL query results. Never hedge or express uncertainty unless the data is incomplete.\"),\n",
        "    (\"human\", \"Question: {question}\\nSQL Query: {query}\\nSQL Result: {result}\\nSchema Context: {schema}\")\n",
        "])\n",
        "\n",
        "rephrase_answer = chat_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_kwargs={\"k\": 3}\n",
        ")\n",
        "retriever_func = lambda x: retriever.invoke(x[\"question\"])\n",
        "\n",
        "\n",
        "sample_input = {\"question\": \"How many accounts have currency of usd\"}\n",
        "retriever_results = retriever_func(sample_input)\n",
        "print(retriever_results)\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(schema=lambda x: retriever.invoke(x[\"question\"]))\n",
        "    .assign(query=generate_query)\n",
        "    .assign(\n",
        "        result=itemgetter(\"query\") | \n",
        "        RunnableLambda(lambda q: safe_execute_query(q))\n",
        "    )\n",
        "    | rephrase_answer\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"How many unique synapsecustomername are there\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (2.2.5)\n",
            "Requirement already satisfied: packaging in c:\\users\\christo.jomon\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl (15.0 MB)\n",
            "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.5/15.0 MB 8.5 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 5.0/15.0 MB 17.8 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 10.2/15.0 MB 21.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  14.9/15.0 MB 21.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 15.0/15.0 MB 21.0 MB/s eta 0:00:00\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading existing enhanced FAISS index...\n",
            "\n",
            "Enhanced FAISS Retrieval Results:\n",
            "\n",
            "Document 1:\n",
            "Content: TABLE: accounts COLUMN: currencyid TYPE: varchar(50)\n",
            "Metadata: {'table': 'accounts', 'column': 'currencyid', 'type': 'column_info'}\n",
            "[Document(id='55ba7727-cac9-43cd-9d58-4041fa1094bb', metadata={'table': 'accounts', 'column': 'currencyid', 'type': 'column_info'}, page_content='TABLE: accounts COLUMN: currencyid TYPE: varchar(50)')]\n"
          ]
        }
      ],
      "source": [
        "# Import Document class\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import re\n",
        "from langchain.chains import create_sql_query_chain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Instead of using default credentials, use an API key approach\n",
        "# Set your API key as an environment variable\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCKHLCrRFIlREEr37RMuqf83E0ezWxdghY\"  # Replace with your actual API key\n",
        "\n",
        "# Create the LLM without credentials parameter\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0\n",
        "    # Don't pass credentials here when using API key\n",
        ")\n",
        "\n",
        "answer_prompt = PromptTemplate.from_template(\n",
        "     \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
        "\n",
        " Question: {question}\n",
        " SQL Query: {query}\n",
        " SQL Result: {result}\n",
        " Answer: \"\"\"\n",
        " )\n",
        "\n",
        "rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
        "# Assuming db is defined elsewhere in your code\n",
        "generate_query = create_sql_query_chain(llm, db)\n",
        "\n",
        "# 3. Enhanced FAISS vector store that includes column information\n",
        "def create_enhanced_faiss_index():\n",
        "    # Initialize embeddings\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "    \n",
        "    # Load your data\n",
        "    df = pd.read_csv(\"./table_schema.csv\")\n",
        "    \n",
        "    # Create documents with enhanced content\n",
        "    documents = []\n",
        "    \n",
        "    for i, row in df.iterrows():\n",
        "        # Extract column definitions from DDL\n",
        "        column_pattern = r'(\\w+)\\s+(\\w+(?:\\(\\d+\\))?)\\s+(\\w+)?'\n",
        "        columns = re.findall(column_pattern, row['DDL'])\n",
        "        \n",
        "        # Create a string of column names and types\n",
        "        column_info = \"\"\n",
        "        for col in columns:\n",
        "            if len(col) >= 2:\n",
        "                col_name, col_type = col[0], col[1]\n",
        "                column_info += f\"{col_name} ({col_type}), \"\n",
        "        \n",
        "        # Create main document for table\n",
        "        table_doc = Document(\n",
        "            page_content=f\"TABLE: {row['table_name']} SCHEMA: {row['DDL']}\",\n",
        "            metadata={\"table\": row[\"table_name\"], \"type\": \"table_schema\"}\n",
        "        )\n",
        "        documents.append(table_doc)\n",
        "        \n",
        "        # Create separate documents for each column for better matching\n",
        "        if columns:\n",
        "            for col in columns:\n",
        "                if len(col) >= 2:\n",
        "                    col_name, col_type = col[0], col[1]\n",
        "                    col_doc = Document(\n",
        "                        page_content=f\"TABLE: {row['table_name']} COLUMN: {col_name} TYPE: {col_type}\",\n",
        "                        metadata={\"table\": row[\"table_name\"], \"column\": col_name, \"type\": \"column_info\"}\n",
        "                    )\n",
        "                    documents.append(col_doc)\n",
        "    \n",
        "    # Create FAISS index\n",
        "    vectorstore = FAISS.from_documents(\n",
        "        documents,\n",
        "        embeddings\n",
        "    )\n",
        "    \n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(\"faiss_index_enhanced\", exist_ok=True)\n",
        "    \n",
        "    # Save the index to disk\n",
        "    vectorstore.save_local(\"faiss_index_enhanced\")\n",
        "    \n",
        "    print(f\"Created enhanced FAISS index with {len(documents)} documents\")\n",
        "    return vectorstore\n",
        "\n",
        "# 4. Load existing enhanced FAISS index or create a new one\n",
        "def get_enhanced_faiss_retriever():\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "    \n",
        "    # Check if index files exist\n",
        "    if os.path.exists(\"faiss_index_enhanced/index.faiss\") and os.path.exists(\"faiss_index_enhanced/index.pkl\"):\n",
        "        print(\"Loading existing enhanced FAISS index...\")\n",
        "        try:\n",
        "            vectorstore = FAISS.load_local(\"faiss_index_enhanced\", embeddings, allow_dangerous_deserialization=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading index: {e}\")\n",
        "            print(\"Creating new enhanced index instead...\")\n",
        "            if os.path.exists(\"faiss_index_enhanced\"):\n",
        "                shutil.rmtree(\"faiss_index_enhanced\")\n",
        "            vectorstore = create_enhanced_faiss_index()\n",
        "    else:\n",
        "        print(\"Creating new enhanced FAISS index...\")\n",
        "        vectorstore = create_enhanced_faiss_index()\n",
        "    \n",
        "    # Create retriever with higher k for better coverage\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
        "    return retriever\n",
        "\n",
        "# 5. Use the enhanced FAISS retriever\n",
        "enhanced_faiss_retriever = get_enhanced_faiss_retriever()\n",
        "\n",
        "# Test the enhanced retriever with column-specific query\n",
        "column_query = \"how many active accounts have curency usd\"  # This should match column names now\n",
        "enhanced_results = enhanced_faiss_retriever.invoke(column_query)\n",
        "\n",
        "print(\"\\nEnhanced FAISS Retrieval Results:\")\n",
        "for i, doc in enumerate(enhanced_results):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "    print(enhanced_faiss_retriever.invoke(column_query))\n",
        "# 6. Update your chain to use the enhanced retriever\n",
        "chain_with_enhanced_retriever = (\n",
        "    RunnablePassthrough.assign(schema=lambda x: enhanced_faiss_retriever.invoke(x[\"question\"]))\n",
        "    .assign(query=generate_query)\n",
        "    .assign(\n",
        "        result=itemgetter(\"query\") | \n",
        "        RunnableLambda(lambda q: safe_execute_query(q))\n",
        "    )\n",
        "    | rephrase_answer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "IVF FAISS Store Statistics:\n",
            "document_count: 210\n",
            "compressed_size_bytes: 21549\n",
            "estimated_uncompressed_bytes: 64701.00000000001\n",
            "compression_ratio: 3.0025059167478774\n",
            "avg_document_size_bytes: 102.61428571428571\n",
            "embedding_dim: 768\n",
            "nlist: 50\n",
            "\n",
            "IVF FAISS Retrieval Results:\n",
            "\n",
            "Document 1:\n",
            "Content: TABLE: container COLUMN: volumeuom TYPE: varchar(10)\n",
            "Metadata: {'table': 'container', 'column': 'volumeuom', 'type': 'column_info'}\n",
            "\n",
            "Document 2:\n",
            "Content: TABLE: container COLUMN: volumemax TYPE: numeric\n",
            "Metadata: {'table': 'container', 'column': 'volumemax', 'type': 'column_info'}\n",
            "\n",
            "Document 3:\n",
            "Content: TABLE: container COLUMN: volumemin TYPE: numeric\n",
            "Metadata: {'table': 'container', 'column': 'volumemin', 'type': 'column_info'}\n",
            "\n",
            "Response with IVF FAISS Retriever:\n",
            "[Document(metadata={'table': 'container', 'column': 'volumemax', 'type': 'column_info'}, page_content='TABLE: container COLUMN: volumemax TYPE: numeric'), Document(metadata={'table': 'container', 'column': 'unitmax', 'type': 'column_info'}, page_content='TABLE: container COLUMN: unitmax TYPE: numeric'), Document(metadata={'table': 'container', 'column': 'volumeuom', 'type': 'column_info'}, page_content='TABLE: container COLUMN: volumeuom TYPE: varchar(10)')]\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import zlib\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class IVFFAISSStore:\n",
        "    \"\"\"\n",
        "    Memory-efficient FAISS storage using IVF (Inverted File Index)\n",
        "    Optimized for schema storage and retrieval\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_dim: int = 768, nlist: int = 100):\n",
        "        \"\"\"\n",
        "        Initialize IVF FAISS store\n",
        "        \n",
        "        Args:\n",
        "            embedding_dim: Dimension of embeddings\n",
        "            nlist: Number of clusters for IVF index (higher = more precise but slower)\n",
        "        \"\"\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.nlist = nlist\n",
        "        self.index = None\n",
        "        self.document_store = {}  # Compressed document storage\n",
        "        self.metadata_store = {}  # Lightweight metadata\n",
        "        self.id_counter = 0\n",
        "        \n",
        "    def _create_ivf_index(self, sample_embeddings: np.ndarray) -> faiss.Index:\n",
        "        \"\"\"Create IVF index optimized for memory efficiency\"\"\"\n",
        "        \n",
        "        # Create quantizer (the index that produces centroids)\n",
        "        quantizer = faiss.IndexFlatL2(self.embedding_dim)\n",
        "        \n",
        "        # Create IVF index with flat storage for vectors\n",
        "        # This is more memory-efficient than standard FAISS\n",
        "        n_clusters = min(self.nlist, len(sample_embeddings) // 10)\n",
        "        index = faiss.IndexIVFFlat(\n",
        "            quantizer,\n",
        "            self.embedding_dim,\n",
        "            n_clusters\n",
        "        )\n",
        "        \n",
        "        # Set search parameters\n",
        "        # Higher values = more accurate but slower\n",
        "        index.nprobe = min(20, n_clusters // 5)\n",
        "        \n",
        "        return index\n",
        "    \n",
        "    def add_documents(self, embeddings: np.ndarray, documents: List[Dict], metadata: List[Dict] = None):\n",
        "        \"\"\"Add documents with embeddings to the IVF store\"\"\"\n",
        "        \n",
        "        if len(embeddings) == 0:\n",
        "            return\n",
        "            \n",
        "        # Create index if it doesn't exist\n",
        "        if self.index is None:\n",
        "            self.index = self._create_ivf_index(embeddings)\n",
        "            \n",
        "            # Train the index (required for IVF)\n",
        "            if not self.index.is_trained:\n",
        "                self.index.train(embeddings.astype('float32'))\n",
        "        \n",
        "        # Add embeddings to index\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "        \n",
        "        # Store compressed documents and metadata\n",
        "        for i in range(len(documents)):\n",
        "            doc_id = self.id_counter + i\n",
        "            \n",
        "            # Compress and store document\n",
        "            self.document_store[doc_id] = self._compress_document(documents[i])\n",
        "            \n",
        "            # Store metadata separately (no compression for fast access)\n",
        "            if metadata and i < len(metadata):\n",
        "                self.metadata_store[doc_id] = metadata[i]\n",
        "            else:\n",
        "                self.metadata_store[doc_id] = {}\n",
        "                \n",
        "        # Update counter\n",
        "        self.id_counter += len(documents)\n",
        "    \n",
        "    def _compress_document(self, document: Dict) -> bytes:\n",
        "        \"\"\"Compress document content\"\"\"\n",
        "        # Convert to JSON and compress\n",
        "        doc_json = json.dumps(document, separators=(',', ':'))  # Minimal JSON\n",
        "        compressed = zlib.compress(doc_json.encode('utf-8'), level=9)\n",
        "        return compressed\n",
        "    \n",
        "    def _decompress_document(self, compressed_doc: bytes) -> Dict:\n",
        "        \"\"\"Decompress document content\"\"\"\n",
        "        doc_json = zlib.decompress(compressed_doc).decode('utf-8')\n",
        "        return json.loads(doc_json)\n",
        "    \n",
        "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search with IVF index\"\"\"\n",
        "        if self.index is None:\n",
        "            return []\n",
        "            \n",
        "        # Search the IVF index\n",
        "        distances, indices = self.index.search(\n",
        "            query_embedding.reshape(1, -1).astype('float32'), k\n",
        "        )\n",
        "        \n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx >= 0 and idx < self.id_counter:  # Valid result\n",
        "                # Get metadata (always available)\n",
        "                metadata = self.metadata_store.get(idx, {})\n",
        "                \n",
        "                # Decompress document only when needed\n",
        "                if idx in self.document_store:\n",
        "                    doc = self._decompress_document(self.document_store[idx])\n",
        "                    content = doc.get('content', '')\n",
        "                else:\n",
        "                    content = f\"Document {idx} not found\"\n",
        "                \n",
        "                results.append({\n",
        "                    'content': content,\n",
        "                    'metadata': metadata,\n",
        "                    'distance': float(distances[0][i]),\n",
        "                    'doc_id': idx\n",
        "                })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Get statistics about the IVF store\"\"\"\n",
        "        if not self.document_store:\n",
        "            return {\"error\": \"No documents stored\"}\n",
        "            \n",
        "        # Calculate stats\n",
        "        total_docs = len(self.document_store)\n",
        "        total_compressed_size = sum(len(doc) for doc in self.document_store.values())\n",
        "        \n",
        "        # Sample a few documents to estimate uncompressed size\n",
        "        sample_size = min(10, total_docs)\n",
        "        if sample_size > 0:\n",
        "            sample_ids = list(self.document_store.keys())[:sample_size]\n",
        "            sample_uncompressed = sum(\n",
        "                len(json.dumps(self._decompress_document(self.document_store[idx])).encode('utf-8'))\n",
        "                for idx in sample_ids\n",
        "            )\n",
        "            estimated_uncompressed = (sample_uncompressed / sample_size) * total_docs\n",
        "            compression_ratio = estimated_uncompressed / total_compressed_size if total_compressed_size > 0 else 0\n",
        "        else:\n",
        "            estimated_uncompressed = 0\n",
        "            compression_ratio = 0\n",
        "            \n",
        "        return {\n",
        "            \"document_count\": total_docs,\n",
        "            \"compressed_size_bytes\": total_compressed_size,\n",
        "            \"estimated_uncompressed_bytes\": estimated_uncompressed,\n",
        "            \"compression_ratio\": compression_ratio,\n",
        "            \"avg_document_size_bytes\": total_compressed_size / total_docs if total_docs > 0 else 0,\n",
        "            \"embedding_dim\": self.embedding_dim,\n",
        "            \"nlist\": self.nlist\n",
        "        }\n",
        "    \n",
        "    def save(self, directory: str):\n",
        "        \"\"\"Save the IVF store to disk\"\"\"\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        \n",
        "        # Save the FAISS index\n",
        "        if self.index is not None:\n",
        "            faiss.write_index(self.index, os.path.join(directory, \"ivf_index.faiss\"))\n",
        "            \n",
        "        # Save document store and metadata\n",
        "        with open(os.path.join(directory, \"documents.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(self.document_store, f)\n",
        "            \n",
        "        with open(os.path.join(directory, \"metadata.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(self.metadata_store, f)\n",
        "            \n",
        "        # Save configuration\n",
        "        config = {\n",
        "            \"embedding_dim\": self.embedding_dim,\n",
        "            \"nlist\": self.nlist,\n",
        "            \"id_counter\": self.id_counter\n",
        "        }\n",
        "        \n",
        "        with open(os.path.join(directory, \"config.json\"), \"w\") as f:\n",
        "            json.dump(config, f)\n",
        "    \n",
        "    @classmethod\n",
        "    def load(cls, directory: str) -> 'IVFFAISSStore':\n",
        "        \"\"\"Load an IVF store from disk\"\"\"\n",
        "        # Load configuration\n",
        "        with open(os.path.join(directory, \"config.json\"), \"r\") as f:\n",
        "            config = json.load(f)\n",
        "            \n",
        "        # Create instance\n",
        "        store = cls(\n",
        "            embedding_dim=config[\"embedding_dim\"],\n",
        "            nlist=config[\"nlist\"]\n",
        "        )\n",
        "        \n",
        "        # Load the FAISS index\n",
        "        store.index = faiss.read_index(os.path.join(directory, \"ivf_index.faiss\"))\n",
        "        \n",
        "        # Load document store and metadata\n",
        "        with open(os.path.join(directory, \"documents.pkl\"), \"rb\") as f:\n",
        "            store.document_store = pickle.load(f)\n",
        "            \n",
        "        with open(os.path.join(directory, \"metadata.pkl\"), \"rb\") as f:\n",
        "            store.metadata_store = pickle.load(f)\n",
        "            \n",
        "        # Set counter\n",
        "        store.id_counter = config[\"id_counter\"]\n",
        "        \n",
        "        return store\n",
        "\n",
        "# Create IVF FAISS store from schema data\n",
        "def create_ivf_schema_store():\n",
        "    \"\"\"Create an IVF FAISS store from schema data\"\"\"\n",
        "    # Initialize embeddings\n",
        "    embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "    \n",
        "    # Load your data\n",
        "    df = pd.read_csv(\"./table_schema.csv\")\n",
        "    \n",
        "    # Prepare documents and metadata\n",
        "    documents = []\n",
        "    metadata = []\n",
        "    \n",
        "    for i, row in df.iterrows():\n",
        "        # Extract column definitions from DDL\n",
        "        column_pattern = r'(\\w+)\\s+(\\w+(?:\\(\\d+\\))?)\\s+(\\w+)?'\n",
        "        columns = re.findall(column_pattern, row['DDL'])\n",
        "        \n",
        "        # Create document for table\n",
        "        table_doc = {\n",
        "            'content': f\"TABLE: {row['table_name']} SCHEMA: {row['DDL']}\",\n",
        "            'type': 'table_schema'\n",
        "        }\n",
        "        \n",
        "        table_meta = {\n",
        "            'table': row['table_name'],\n",
        "            'type': 'table_schema'\n",
        "        }\n",
        "        \n",
        "        documents.append(table_doc)\n",
        "        metadata.append(table_meta)\n",
        "        \n",
        "        # Create documents for each column\n",
        "        for col in columns:\n",
        "            if len(col) >= 2:\n",
        "                col_name, col_type = col[0], col[1]\n",
        "                col_doc = {\n",
        "                    'content': f\"TABLE: {row['table_name']} COLUMN: {col_name} TYPE: {col_type}\",\n",
        "                    'type': 'column_info'\n",
        "                }\n",
        "                \n",
        "                col_meta = {\n",
        "                    'table': row['table_name'],\n",
        "                    'column': col_name,\n",
        "                    'type': 'column_info'\n",
        "                }\n",
        "                \n",
        "                documents.append(col_doc)\n",
        "                metadata.append(col_meta)\n",
        "    \n",
        "    # Get embeddings for all documents\n",
        "    texts = [doc['content'] for doc in documents]\n",
        "    embeddings = np.array(embeddings_model.embed_documents(texts))\n",
        "    \n",
        "    # Create IVF store\n",
        "    store = IVFFAISSStore(embedding_dim=embeddings.shape[1], nlist=50)\n",
        "    store.add_documents(embeddings, documents, metadata)\n",
        "    \n",
        "    # Save the store\n",
        "    store.save(\"ivf_schema_store\")\n",
        "    \n",
        "    # Show stats\n",
        "    stats = store.get_stats()\n",
        "    print(\"\\nIVF FAISS Store Statistics:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    \n",
        "    return store\n",
        "\n",
        "# LangChain-compatible retriever for IVF store\n",
        "class IVFFAISSRetriever:\n",
        "    \"\"\"LangChain-compatible retriever for IVF FAISS store\"\"\"\n",
        "    \n",
        "    def __init__(self, store: IVFFAISSStore):\n",
        "        self.store = store\n",
        "        self.embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "    \n",
        "    def invoke(self, query: str) -> List[Document]:\n",
        "        \"\"\"LangChain-compatible invoke method\"\"\"\n",
        "        # Get query embedding\n",
        "        query_embedding = np.array(self.embeddings_model.embed_query(query))\n",
        "        \n",
        "        # Search\n",
        "        results = self.store.search(query_embedding, k=3)\n",
        "        \n",
        "        # Convert to LangChain Document format\n",
        "        documents = []\n",
        "        for result in results:\n",
        "            documents.append(Document(\n",
        "                page_content=result['content'],\n",
        "                metadata=result['metadata']\n",
        "            ))\n",
        "        \n",
        "        return documents\n",
        "\n",
        "# Create and test the IVF store\n",
        "ivf_store = create_ivf_schema_store()\n",
        "ivf_retriever = IVFFAISSRetriever(ivf_store)\n",
        "\n",
        "# Test the IVF retriever\n",
        "test_query = \"volume\"\n",
        "ivf_results = ivf_retriever.invoke(test_query)\n",
        "\n",
        "print(\"\\nIVF FAISS Retrieval Results:\")\n",
        "for i, doc in enumerate(ivf_results):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "\n",
        "# Update your chain to use the IVF retriever\n",
        "# chain_with_ivf_retriever = (\n",
        "#     RunnablePassthrough.assign(schema=lambda x: ivf_retriever.invoke(x[\"question\"]))\n",
        "#     .assign(query=generate_query)\n",
        "#     .assign(\n",
        "#         result=itemgetter(\"query\") | \n",
        "#         RunnableLambda(lambda q: safe_execute_query(q))\n",
        "#     )\n",
        "#     | rephrase_answer\n",
        "# )\n",
        "\n",
        "# Test the chain with IVF retriever\n",
        "response = ivf_retriever.invoke(\"max volume\")\n",
        "print(\"\\nResponse with IVF FAISS Retriever:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Table Schema Results:\n",
            "\n",
            "Document 1:\n",
            "Content: TABLE: container\n",
            "CREATE TABLE customersetup.container (\n",
            "\tid varchar(40) NOT NULL,\n",
            "\tbusinessunitid varchar(40) NULL,\n",
            "\twarehouseid varchar(40) NULL,\n",
            "\taccountid varchar(40) NULL,\n",
            "\tcontainerclass varchar(40) NULL,\n",
            "\tcontainegroup varchar(40) NULL,\n",
            "\tcontainername varchar(40) NULL,\n",
            "\tdescription varchar(40) NULL,\n",
            "\tvolumemin numeric NULL,\n",
            "\tvolumemax numeric NULL,\n",
            "\tvolumeuom varchar(10) NULL,\n",
            "\tweightmin numeric NULL,\n",
            "\tweightmax numeric NULL,\n",
            "\tweightuom varchar(10) NULL,\n",
            "\tunitmax numeric NULL,\n",
            "\tunituom varchar(10) NULL,\n",
            "\tdimensionslength numeric NULL,\n",
            "\tdimensionswidth numeric NULL,\n",
            "\tdimensionsheight numeric NULL,\n",
            "\tdimensionsuom varchar(10) NULL,\n",
            "\tcreatedby varchar(250) NULL,\n",
            "\tcreateddate timestamp(6) NULL,\n",
            "\tmodifiedby varchar(250) NULL,\n",
            "\tmodifieddate timestamp(6) NULL,\n",
            "\ttimeinepoch numeric NULL,\n",
            "\tisdeleted bpchar(1) NULL,\n",
            "\tisarchived bpchar(1) NULL,\n",
            "\tcurrentappversion varchar(7) NULL,\n",
            "\tmodifiedappversion varchar(7) NULL,\n",
            "\tcreatedlocation varchar(200) NULL,\n",
            "\tupdatedlocation varchar(200) NULL,\n",
            "\tcustomerid varchar(50) NULL,\n",
            "\tCONSTRAINT container_pk PRIMARY KEY (id)\n",
            ");\n",
            "Metadata: {'table': 'container'}\n",
            "\n",
            "Response with Table Schema Retriever:\n",
            "[Document(id='4c563bd0-5e1c-4534-a801-3dbd94f3dcc3', metadata={'table': 'container'}, page_content='TABLE: container\\nCREATE TABLE customersetup.container (\\r\\n\\tid varchar(40) NOT NULL,\\r\\n\\tbusinessunitid varchar(40) NULL,\\r\\n\\twarehouseid varchar(40) NULL,\\r\\n\\taccountid varchar(40) NULL,\\r\\n\\tcontainerclass varchar(40) NULL,\\r\\n\\tcontainegroup varchar(40) NULL,\\r\\n\\tcontainername varchar(40) NULL,\\r\\n\\tdescription varchar(40) NULL,\\r\\n\\tvolumemin numeric NULL,\\r\\n\\tvolumemax numeric NULL,\\r\\n\\tvolumeuom varchar(10) NULL,\\r\\n\\tweightmin numeric NULL,\\r\\n\\tweightmax numeric NULL,\\r\\n\\tweightuom varchar(10) NULL,\\r\\n\\tunitmax numeric NULL,\\r\\n\\tunituom varchar(10) NULL,\\r\\n\\tdimensionslength numeric NULL,\\r\\n\\tdimensionswidth numeric NULL,\\r\\n\\tdimensionsheight numeric NULL,\\r\\n\\tdimensionsuom varchar(10) NULL,\\r\\n\\tcreatedby varchar(250) NULL,\\r\\n\\tcreateddate timestamp(6) NULL,\\r\\n\\tmodifiedby varchar(250) NULL,\\r\\n\\tmodifieddate timestamp(6) NULL,\\r\\n\\ttimeinepoch numeric NULL,\\r\\n\\tisdeleted bpchar(1) NULL,\\r\\n\\tisarchived bpchar(1) NULL,\\r\\n\\tcurrentappversion varchar(7) NULL,\\r\\n\\tmodifiedappversion varchar(7) NULL,\\r\\n\\tcreatedlocation varchar(200) NULL,\\r\\n\\tupdatedlocation varchar(200) NULL,\\r\\n\\tcustomerid varchar(50) NULL,\\r\\n\\tCONSTRAINT container_pk PRIMARY KEY (id)\\r\\n);')]\n"
          ]
        }
      ],
      "source": [
        "class TableSchemaRetriever:\n",
        "    \"\"\"\n",
        "    Simple vector store solution that retrieves only complete table schemas\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "        # Use FAISS from langchain which has better error handling\n",
        "        from langchain_community.vectorstores import FAISS\n",
        "        self.FAISS_cls = FAISS\n",
        "        self.vector_store = self._initialize_store()\n",
        "        \n",
        "    def _initialize_store(self):\n",
        "        \"\"\"Initialize the vector store\"\"\"\n",
        "        from langchain_community.vectorstores import FAISS\n",
        "        \n",
        "        # Check if index exists\n",
        "        if os.path.exists(\"faiss_index/index.faiss\") and os.path.exists(\"faiss_index/index.pkl\"):\n",
        "            try:\n",
        "                # Load existing index\n",
        "                return FAISS.load_local(\"faiss_index\", self.embeddings_model, allow_dangerous_deserialization=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading index: {e}\")\n",
        "                print(\"Creating new index instead...\")\n",
        "        \n",
        "        # Create new index\n",
        "        return self._create_store()\n",
        "    \n",
        "    def _create_store(self):\n",
        "        \"\"\"Create vector store with table schemas only\"\"\"\n",
        "        # Load data\n",
        "        df = pd.read_csv(\"./table_schema.csv\")\n",
        "        \n",
        "        # Prepare documents - only table schemas\n",
        "        documents = []\n",
        "        \n",
        "        for i, row in df.iterrows():\n",
        "            # Create document with full schema\n",
        "            doc = Document(\n",
        "                page_content=f\"TABLE: {row['table_name']}\\n{row['DDL']}\",\n",
        "                metadata={\"table\": row['table_name']}\n",
        "            )\n",
        "            documents.append(doc)\n",
        "        \n",
        "        # Create FAISS index\n",
        "        vector_store = self.FAISS_cls.from_documents(\n",
        "            documents,\n",
        "            self.embeddings_model\n",
        "        )\n",
        "        \n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(\"faiss_index\", exist_ok=True)\n",
        "        \n",
        "        # Save the index to disk\n",
        "        vector_store.save_local(\"faiss_index\")\n",
        "        \n",
        "        return vector_store\n",
        "    \n",
        "    def retrieve(self, query, k=1):\n",
        "        \"\"\"Retrieve relevant table schemas\"\"\"\n",
        "        # Use the built-in retriever\n",
        "        retriever = self.vector_store.as_retriever(search_kwargs={\"k\": k})\n",
        "        return retriever.invoke(query)\n",
        "\n",
        "# Create the table schema retriever\n",
        "schema_retriever = TableSchemaRetriever()\n",
        "\n",
        "# Test the retriever\n",
        "test_query = \"max volume\"\n",
        "schema_results = schema_retriever.retrieve(test_query)\n",
        "\n",
        "print(\"\\nTable Schema Results:\")\n",
        "for i, doc in enumerate(schema_results):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "\n",
        "# Update chain to use the table schema retriever\n",
        "\n",
        "\n",
        "# Test the chain\n",
        "response = schema_retriever.retrieve(\"volume\")\n",
        "print(\"\\nResponse with Table Schema Retriever:\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
