{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ce6e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import zlib\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.tools import QuerySQLDataBaseTool\n",
    "\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyBc_8Ls8yQQsgOgeMusRW3Y8jcC3EO1E_k\"\n",
    "\n",
    "db_user = \"usr_reporting\"\n",
    "db_password = \"Atdd5v3ecsr3p\"\n",
    "db_host = \"alspgbdvit01q.ohl.com\"\n",
    "db_name = \"vite_reporting_r_qa\"\n",
    "db_port = 6432\n",
    "db_schema = \"customersetup\"  # Define the schema name\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the engine without specifying the schema in the URI\n",
    "engine = create_engine(f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\")\n",
    "\n",
    "# Create the SQLDatabase object, specifying the schema\n",
    "db = SQLDatabase(engine, schema=db_schema)\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCKHLCrRFIlREEr37RMuqf83E0ezWxdghY\"  # Replace with your actual API key\n",
    "\n",
    "# Create the LLM without credentials parameter\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0\n",
    "    # Don't pass credentials here when using API key\n",
    ")\n",
    "\n",
    "sql_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"You are an SQL expert. Generate SQL queries based on the user's question and provided schema context.\n",
    "Follow these rules:\n",
    "1. Use only tables and columns mentioned in the schema context\n",
    "2. Write clear, efficient SQL queries\n",
    "3. Consider table relationships and column types from the schema\"\"\"),\n",
    "    (\"human\", \"Schema Context: {schema}\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Create SQL generation chain without database dependency\n",
    "generate_query = (sql_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "def safe_execute_query(query):\n",
    "    try:\n",
    "        print(\"query:\", strip_sql_markdown(query))\n",
    "        return execute_query.invoke(strip_sql_markdown(query))\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {str(e)}\"\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "def strip_sql_markdown(sql: str) -> str:\n",
    "    return sql.strip().replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Rephrase the answer to the question based on the from LLM and schema context.\"),\n",
    "    (\"human\", \"Question: {question}\\nSQL Query: {query}\\nSQL Result: {result}\\nSchema Context: {schema}\")\n",
    "])\n",
    "\n",
    "rephrase_answer = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "class IVFFAISSStore:\n",
    "    \"\"\"\n",
    "    Memory-efficient FAISS storage using IVF (Inverted File Index)\n",
    "    Optimized for schema storage and retrieval\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 768, nlist: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize IVF FAISS store\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of embeddings\n",
    "            nlist: Number of clusters for IVF index (higher = more precise but slower)\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.nlist = nlist\n",
    "        self.index = None\n",
    "        self.document_store = {}  # Compressed document storage\n",
    "        self.metadata_store = {}  # Lightweight metadata\n",
    "        self.id_counter = 0\n",
    "        \n",
    "    def _create_ivf_index(self, sample_embeddings: np.ndarray) -> faiss.Index:\n",
    "        \"\"\"Create IVF index optimized for memory efficiency\"\"\"\n",
    "        \n",
    "        # Create quantizer (the index that produces centroids)\n",
    "        quantizer = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        \n",
    "        # Create IVF index with flat storage for vectors\n",
    "        # This is more memory-efficient than standard FAISS\n",
    "        n_clusters = min(self.nlist, len(sample_embeddings) // 10)\n",
    "        index = faiss.IndexIVFFlat(\n",
    "            quantizer,\n",
    "            self.embedding_dim,\n",
    "            n_clusters\n",
    "        )\n",
    "        \n",
    "        # Set search parameters\n",
    "        # Higher values = more accurate but slower\n",
    "        index.nprobe = min(20, n_clusters // 5)\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def add_documents(self, embeddings: np.ndarray, documents: List[Dict], metadata: List[Dict] = None):\n",
    "        \"\"\"Add documents with embeddings to the IVF store\"\"\"\n",
    "        \n",
    "        if len(embeddings) == 0:\n",
    "            return\n",
    "            \n",
    "        # Create index if it doesn't exist\n",
    "        if self.index is None:\n",
    "            self.index = self._create_ivf_index(embeddings)\n",
    "            \n",
    "            # Train the index (required for IVF)\n",
    "            if not self.index.is_trained:\n",
    "                self.index.train(embeddings.astype('float32'))\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        # Store compressed documents and metadata\n",
    "        for i in range(len(documents)):\n",
    "            doc_id = self.id_counter + i\n",
    "            \n",
    "            # Compress and store document\n",
    "            self.document_store[doc_id] = self._compress_document(documents[i])\n",
    "            \n",
    "            # Store metadata separately (no compression for fast access)\n",
    "            if metadata and i < len(metadata):\n",
    "                self.metadata_store[doc_id] = metadata[i]\n",
    "            else:\n",
    "                self.metadata_store[doc_id] = {}\n",
    "                \n",
    "        # Update counter\n",
    "        self.id_counter += len(documents)\n",
    "    \n",
    "    def _compress_document(self, document: Dict) -> bytes:\n",
    "        \"\"\"Compress document content\"\"\"\n",
    "        # Convert to JSON and compress\n",
    "        doc_json = json.dumps(document, separators=(',', ':'))  # Minimal JSON\n",
    "        compressed = zlib.compress(doc_json.encode('utf-8'), level=9)\n",
    "        return compressed\n",
    "    \n",
    "    def _decompress_document(self, compressed_doc: bytes) -> Dict:\n",
    "        \"\"\"Decompress document content\"\"\"\n",
    "        doc_json = zlib.decompress(compressed_doc).decode('utf-8')\n",
    "        return json.loads(doc_json)\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search with IVF index\"\"\"\n",
    "        if self.index is None:\n",
    "            return []\n",
    "            \n",
    "        # Search the IVF index\n",
    "        distances, indices = self.index.search(\n",
    "            query_embedding.reshape(1, -1).astype('float32'), k\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx >= 0 and idx < self.id_counter:  # Valid result\n",
    "                # Get metadata (always available)\n",
    "                metadata = self.metadata_store.get(idx, {})\n",
    "                \n",
    "                # Decompress document only when needed\n",
    "                if idx in self.document_store:\n",
    "                    doc = self._decompress_document(self.document_store[idx])\n",
    "                    content = doc.get('content', '')\n",
    "                else:\n",
    "                    content = f\"Document {idx} not found\"\n",
    "                \n",
    "                results.append({\n",
    "                    'content': content,\n",
    "                    'metadata': metadata,\n",
    "                    'distance': float(distances[0][i]),\n",
    "                    'doc_id': idx\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the IVF store\"\"\"\n",
    "        if not self.document_store:\n",
    "            return {\"error\": \"No documents stored\"}\n",
    "            \n",
    "        # Calculate stats\n",
    "        total_docs = len(self.document_store)\n",
    "        total_compressed_size = sum(len(doc) for doc in self.document_store.values())\n",
    "        \n",
    "        # Sample a few documents to estimate uncompressed size\n",
    "        sample_size = min(10, total_docs)\n",
    "        if sample_size > 0:\n",
    "            sample_ids = list(self.document_store.keys())[:sample_size]\n",
    "            sample_uncompressed = sum(\n",
    "                len(json.dumps(self._decompress_document(self.document_store[idx])).encode('utf-8'))\n",
    "                for idx in sample_ids\n",
    "            )\n",
    "            estimated_uncompressed = (sample_uncompressed / sample_size) * total_docs\n",
    "            compression_ratio = estimated_uncompressed / total_compressed_size if total_compressed_size > 0 else 0\n",
    "        else:\n",
    "            estimated_uncompressed = 0\n",
    "            compression_ratio = 0\n",
    "            \n",
    "        return {\n",
    "            \"document_count\": total_docs,\n",
    "            \"compressed_size_bytes\": total_compressed_size,\n",
    "            \"estimated_uncompressed_bytes\": estimated_uncompressed,\n",
    "            \"compression_ratio\": compression_ratio,\n",
    "            \"avg_document_size_bytes\": total_compressed_size / total_docs if total_docs > 0 else 0,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"nlist\": self.nlist\n",
    "        }\n",
    "    \n",
    "    def save(self, directory: str):\n",
    "        \"\"\"Save the IVF store to disk\"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # Save the FAISS index\n",
    "        if self.index is not None:\n",
    "            faiss.write_index(self.index, os.path.join(directory, \"ivf_index.faiss\"))\n",
    "            \n",
    "        # Save document store and metadata\n",
    "        with open(os.path.join(directory, \"documents.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.document_store, f)\n",
    "            \n",
    "        with open(os.path.join(directory, \"metadata.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.metadata_store, f)\n",
    "            \n",
    "        # Save configuration\n",
    "        config = {\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"nlist\": self.nlist,\n",
    "            \"id_counter\": self.id_counter\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(directory, \"config.json\"), \"w\") as f:\n",
    "            json.dump(config, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, directory: str) -> 'IVFFAISSStore':\n",
    "        \"\"\"Load an IVF store from disk\"\"\"\n",
    "        # Load configuration\n",
    "        with open(os.path.join(directory, \"config.json\"), \"r\") as f:\n",
    "            config = json.load(f)\n",
    "            \n",
    "        # Create instance\n",
    "        store = cls(\n",
    "            embedding_dim=config[\"embedding_dim\"],\n",
    "            nlist=config[\"nlist\"]\n",
    "        )\n",
    "        \n",
    "        # Load the FAISS index\n",
    "        store.index = faiss.read_index(os.path.join(directory, \"ivf_index.faiss\"))\n",
    "        \n",
    "        # Load document store and metadata\n",
    "        with open(os.path.join(directory, \"documents.pkl\"), \"rb\") as f:\n",
    "            store.document_store = pickle.load(f)\n",
    "            \n",
    "        with open(os.path.join(directory, \"metadata.pkl\"), \"rb\") as f:\n",
    "            store.metadata_store = pickle.load(f)\n",
    "            \n",
    "        # Set counter\n",
    "        store.id_counter = config[\"id_counter\"]\n",
    "        \n",
    "        return store\n",
    "\n",
    "# Create IVF FAISS store from schema data\n",
    "def create_ivf_schema_store():\n",
    "    \"\"\"Create an IVF FAISS store from schema data\"\"\"\n",
    "    # Initialize embeddings\n",
    "    embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "    \n",
    "    # Load your data\n",
    "    df = pd.read_csv(\"./table_schema.csv\")\n",
    "    \n",
    "    # Prepare documents and metadata\n",
    "    documents = []\n",
    "    metadata = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        # Extract column definitions from DDL\n",
    "        column_pattern = r'(\\w+)\\s+(\\w+(?:\\(\\d+\\))?)\\s+(\\w+)?'\n",
    "        columns = re.findall(column_pattern, row['DDL'])\n",
    "        \n",
    "        # Create document for table\n",
    "        table_doc = {\n",
    "            'content': f\"TABLE: {row['table_name']} SCHEMA: {row['DDL']}\",\n",
    "            'type': 'table_schema'\n",
    "        }\n",
    "        \n",
    "        table_meta = {\n",
    "            'table': row['table_name'],\n",
    "            'type': 'table_schema'\n",
    "        }\n",
    "        \n",
    "        documents.append(table_doc)\n",
    "        metadata.append(table_meta)\n",
    "        \n",
    "        # Create documents for each column\n",
    "        for col in columns:\n",
    "            if len(col) >= 2:\n",
    "                col_name, col_type = col[0], col[1]\n",
    "                col_doc = {\n",
    "                    'content': f\"TABLE: {row['table_name']} COLUMN: {col_name} TYPE: {col_type}\",\n",
    "                    'type': 'column_info'\n",
    "                }\n",
    "                \n",
    "                col_meta = {\n",
    "                    'table': row['table_name'],\n",
    "                    'column': col_name,\n",
    "                    'type': 'column_info'\n",
    "                }\n",
    "                \n",
    "                documents.append(col_doc)\n",
    "                metadata.append(col_meta)\n",
    "    \n",
    "    # Get embeddings for all documents\n",
    "    texts = [doc['content'] for doc in documents]\n",
    "    embeddings = np.array(embeddings_model.embed_documents(texts))\n",
    "    \n",
    "    # Create IVF store\n",
    "    store = IVFFAISSStore(embedding_dim=embeddings.shape[1], nlist=50)\n",
    "    store.add_documents(embeddings, documents, metadata)\n",
    "    \n",
    "    # Save the store\n",
    "    store.save(\"ivf_schema_store\")\n",
    "    \n",
    "    # Show stats\n",
    "    stats = store.get_stats()\n",
    "    # print(\"\\nIVF FAISS Store Statistics:\")\n",
    "    # for key, value in stats.items():\n",
    "    #     print(f\"{key}: {value}\")\n",
    "    \n",
    "    return store\n",
    "\n",
    "# LangChain-compatible retriever for IVF store\n",
    "class IVFFAISSRetriever:\n",
    "    \"\"\"LangChain-compatible retriever for IVF FAISS store\"\"\"\n",
    "    \n",
    "    def __init__(self, store: IVFFAISSStore):\n",
    "        self.store = store\n",
    "        self.embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "    \n",
    "    def invoke(self, query: str) -> List[Document]:\n",
    "        \"\"\"LangChain-compatible invoke method\"\"\"\n",
    "        # Get query embedding\n",
    "        query_embedding = np.array(self.embeddings_model.embed_query(query))\n",
    "        \n",
    "        # Search\n",
    "        results = self.store.search(query_embedding, k=5)\n",
    "        \n",
    "        # Convert to LangChain Document format\n",
    "        documents = []\n",
    "        for result in results:\n",
    "            documents.append(Document(\n",
    "                page_content=result['content'],\n",
    "                metadata=result['metadata']\n",
    "            ))\n",
    "        \n",
    "        return documents\n",
    "\n",
    "# Create and test the IVF store\n",
    "ivf_store = create_ivf_schema_store()\n",
    "ivf_retriever = IVFFAISSRetriever(ivf_store)\n",
    "\n",
    "# Test the IVF retriever\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ffe937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retreviever   [Document(metadata={'table': 'locations', 'column': 'weightcapacity', 'type': 'column_info'}, page_content='TABLE: locations COLUMN: weightcapacity TYPE: int4'), Document(metadata={'table': 'container', 'column': 'weightuom', 'type': 'column_info'}, page_content='TABLE: container COLUMN: weightuom TYPE: varchar(10)'), Document(metadata={'table': 'container', 'column': 'weightmin', 'type': 'column_info'}, page_content='TABLE: container COLUMN: weightmin TYPE: numeric'), Document(metadata={'table': 'container', 'column': 'weightmax', 'type': 'column_info'}, page_content='TABLE: container COLUMN: weightmax TYPE: numeric'), Document(metadata={'table': 'container', 'column': 'unituom', 'type': 'column_info'}, page_content='TABLE: container COLUMN: unituom TYPE: varchar(10)')]\n",
      "query: SELECT\n",
      "  COUNT(CASE WHEN weightuom = 'Pound' THEN 1 ELSE NULL END)\n",
      "FROM container;\n",
      "\n",
      "Response with IVF FAISS Retriever:\n",
      "There are 9 containers with a weight unit of measure in pounds.\n"
     ]
    }
   ],
   "source": [
    "chain_with_ivf_retriever = (\n",
    "    RunnablePassthrough()\n",
    "    .assign(\n",
    "        schema=lambda x: ivf_retriever.invoke(x[\"question\"]) if isinstance(x, dict) else ivf_retriever.invoke(x)\n",
    "    )\n",
    "    .assign(query=generate_query)\n",
    "    .assign(\n",
    "        result=itemgetter(\"query\") | \n",
    "        RunnableLambda(lambda q: safe_execute_query(q))\n",
    "    )\n",
    "    | rephrase_answer\n",
    ")\n",
    "while True:\n",
    "    try:\n",
    "        question = input(\"Enter your question (or 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "    except EOFError:\n",
    "        break\n",
    "    # Test the chain with a properly formatted input dictionary\n",
    "    response = chain_with_ivf_retriever.invoke({\"question\": question})\n",
    "    print(\"\\nResponse with IVF FAISS Retriever:\")\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
